{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jx6lm9jFwlnU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,balanced_accuracy_score,roc_auc_score\n",
        "import os.path\n",
        "from nltk.stem import SnowballStemmer, PorterStemmer\n",
        "import nltk\n",
        "import re\n",
        "import json\n",
        "import os.path\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_D31Oj6xthO",
        "outputId": "a014e034-14fb-43ce-a6c6-bdc2806fca87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "data = pd.read_csv(\"drive/MyDrive/toxic_classification.csv\")\n",
        "df_0 = data[data['target'] == 0].iloc[:20000, :]\n",
        "df_1 = data[data['target'] == 1].iloc[:20000, :]\n",
        "df_small = pd.concat([df_0, df_1])\n",
        "nltk.download(\"popular\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z0jmkiqE2Zi9"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "y = df_small['target'].astype('int').to_numpy()\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text_string):\n",
        "    text_string = text_string.lower() \n",
        "    text_string = re.sub('[^A-Za-z0-9]+', ' ', text_string)     \n",
        "    x = text_string.split()\n",
        "    new_text = []    \n",
        "    for word in x:\n",
        "        if word not in stop_words:\n",
        "            new_text.append(stemmer.stem(word))          \n",
        "    text_string = ' '.join(new_text)\n",
        "    return text_string\n",
        "if os.path.isfile(\"drive/MyDrive/small_preprocessX\") !=True:\n",
        "    processed_X = [preprocess(i) for i in data.comment_text]\n",
        "    with open(\"drive/MyDrive/small_preprocessX\", \"w\") as fp:\n",
        "        json.dump(processed_X, fp)\n",
        "else:\n",
        "    with open('drive/MyDrive/small_preprocessX', \"r\") as fp:\n",
        "        processed_X = json.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bnsZtQ_yU9VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67cf32a6-1b99-4a99-b454-03153b8e5e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 99.9% 1660.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(processed_X,y, test_size=0.2, random_state=0)\n",
        "# load the Word2Vec model\n",
        "from gensim.models import KeyedVectors\n",
        "filename = 'word2vec-google-news-300'\n",
        "w2v_model = gensim.downloader.load(filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "time.perf_counter()\n",
        "w2v_mode2 = Word2Vec(sentences, vector_size=300,sg=1, window=5, min_count=5) \n",
        "time.perf_counter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOVtYagix4bd",
        "outputId": "3959f961-0be4-43f4-f0b8-ac767187ba64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "691.947090952"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-KXXe-ogVIbe"
      },
      "outputs": [],
      "source": [
        "#w2v_model.save(\"drive/MyDrive/small_w2v_size100_window5_min5_skipgram.model\")\n",
        "#w2v_mode2 =Word2Vec.load(\"drive/MyDrive/small_w2v_size100_window5_min5_skipgram.model\")\n",
        "#w2v_model = Word2Vec.load(\"drive/MyDrive/small_w2v_size100_window5_min5_workers4.model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=time.perf_counter()\n",
        "tf_idf_vectorizer = TfidfVectorizer(token_pattern= r'\\S+')\n",
        "X = tf_idf_vectorizer.fit_transform(processed_X)\n",
        "b=time.perf_counter()\n",
        "print(\"Time: \"+ str(b-a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rBfcQSSmAIu",
        "outputId": "2560517d-089b-4330-d253-47121d8376e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 1.0742212159999553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7g4vrc2mANcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_array = X.toarray()"
      ],
      "metadata": {
        "id": "Hi8py1eDvinQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_set = tf_idf_vectorizer.get_feature_names_out()\n",
        "df_tf_idf = pd.DataFrame(X_array, columns = words_set)"
      ],
      "metadata": {
        "id": "hdJlTzlHmALd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5uspkpt8ZczU"
      },
      "outputs": [],
      "source": [
        "# Vectorize data\n",
        "def vectorize(sentence,modelwv):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [modelwv[word] for word in words if word in modelwv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(300)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_weighted(i,sentence,modelwv):\n",
        "    words_vecs = [modelwv[word]*df_tf_idf[word].loc[i] for word in sentence if word in modelwv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(300)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)"
      ],
      "metadata": {
        "id": "Sgm651N0qa04"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V-fldrVK5yny"
      },
      "outputs": [],
      "source": [
        "#vectorizer = TfidfVectorizer()\n",
        "#X_tfidf = vectorizer.fit_transform(processed_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0NzwRGRNATpt"
      },
      "outputs": [],
      "source": [
        "#X_train_s = np.array([vectorize(sentence,w2v_mode2.wv) for sentence in X_train])\n",
        "#X_test_s = np.array([vectorize(sentence,w2v_mode2.wv) for sentence in X_test])\n",
        "X_train_s = np.array([vectorize(sentence,w2v_model) for sentence in X_train])\n",
        "X_test_s = np.array([vectorize(sentence,w2v_model) for sentence in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [sentence.split() for sentence in processed_X]\n",
        "#X_w = np.array([vectorize_weighted(i,sentence,w2v_mode2.wv) for i,sentence in enumerate(sentences)])\n",
        "X_w = np.array([vectorize_weighted(i,sentence,w2v_model) for i,sentence in enumerate(sentences)])\n",
        "X_train_sw, X_test_sw, y_train, y_test = train_test_split(X_w,y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "9UIzvySe0836"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWarCKL0Usvs",
        "outputId": "0cec8591-18fe-4b1f-d154-f5eadffd4930"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(X_train_s[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz0ij-ZwGpU6",
        "outputId": "0919d766-7edb-4d32-d7da-9eacca758e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]Time: 614.5006338830001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 10.2min finished\n"
          ]
        }
      ],
      "source": [
        "a=time.perf_counter()\n",
        "bagging_SVM_weighted = BaggingClassifier(SVC(kernel= 'linear', random_state=1, C=0.1,verbose=True),n_estimators=5,max_samples=0.99,verbose=True)\n",
        "bagging_SVM_weighted.fit(X_train_sw, y_train)\n",
        "b=time.perf_counter()\n",
        "print(\"Time: \"+ str(b-a))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=time.perf_counter()\n",
        "bagging_SVM_s = BaggingClassifier(SVC(kernel= 'linear', random_state=1, C=0.1,verbose=True),n_estimators=5,max_samples=0.99,verbose=True)\n",
        "bagging_SVM_s.fit(X_train_s, y_train)\n",
        "b=time.perf_counter()\n",
        "print(\"Time: \"+ str(b-a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwNLNN748KIz",
        "outputId": "5924693b-0d33-4a53-84ba-005d259f2c37"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]Time: 482.24718326899983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.0min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlcPb5pCMcV3",
        "outputId": "409904ca-d33d-4d44-cad0-0c3c3508f1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 152.309637029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        }
      ],
      "source": [
        "a=time.perf_counter()\n",
        "y_bg_svm = bagging_SVM_weighted.predict(X_test_sw)\n",
        "b=time.perf_counter()\n",
        "print(\"Time: \"+ str(b-a))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=time.perf_counter()\n",
        "y_bg_svm_s = bagging_SVM_s.predict(X_test_s)\n",
        "b=time.perf_counter()\n",
        "print(\"Time: \" +str(b-a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzvFh-pO8Pbm",
        "outputId": "8cd3763f-a5f7-4355-c80c-05e8934e5385"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 105.120168939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKXsUVwKMdGs",
        "outputId": "423e234e-a34d-49b8-a2d4-b4c1cf2213a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.700\n",
            "Precision: 0.8045801526717558\n",
            "Recall: 0.5275275275275275\n",
            "F1: 0.637\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_bg_svm))\n",
        "#print('Balanced Accuracy: %.3f' %balanced_accuracy_score(y_test, y_bg_svm))\n",
        "print('Precision:', precision_score(y_test, y_bg_svm))\n",
        "print('Recall:', recall_score(y_test, y_bg_svm))\n",
        "print('F1: %.3f' % f1_score(y_test, y_bg_svm))\n",
        "#print('Roc_auc: %.3f' % roc_auc_score(y_test, y_bg_svm))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_bg_svm_s))\n",
        "#print('Balanced Accuracy: %.3f' %balanced_accuracy_score(y_test, y_bg_svm_s))\n",
        "print('Precision:', precision_score(y_test, y_bg_svm_s))\n",
        "print('Recall:', recall_score(y_test, y_bg_svm_s))\n",
        "print('F1: %.3f' % f1_score(y_test, y_bg_svm_s))\n",
        "#print('Roc_auc: %.3f' % roc_auc_score(y_test, y_bg_svm_s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1IpFY0l8Rme",
        "outputId": "ea18e22a-759c-4dfe-c481-4205eea16671"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.792\n",
            "Precision: 0.8126338329764454\n",
            "Recall: 0.7597597597597597\n",
            "F1: 0.785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0HyKXLaiUdp_"
      },
      "outputs": [],
      "source": [
        "#pipe = make_pipeline(StandardScaler(with_mean=True), LogisticRegression())\n",
        "#bagging_LR = BaggingClassifier(pipe,n_estimators=5,max_samples=0.99,verbose=True)\n",
        "#bagging_LR.fit(X_train_sw, y_train)\n",
        "#y_bg_LR = bagging_LR.predict(X_test_sw)\n",
        "\n",
        "#bagging_LR_s = BaggingClassifier(pipe,n_estimators=5,max_samples=0.99,verbose=True)\n",
        "#bagging_LR_s.fit(X_train_s, y_train)\n",
        "#y_bg_LR_s = bagging_LR_s.predict(X_test_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "78Iy9LUeji9E"
      },
      "outputs": [],
      "source": [
        "#print('Accuracy: %.3f' % accuracy_score(y_test, y_bg_LR))\n",
        "#print('Balanced Accuracy: %.3f' %balanced_accuracy_score(y_test, y_bg_LR))\n",
        "###print('Precision:', precision_score(y_test, y_bg_LR))\n",
        "#print('Recall:', recall_score(y_test, y_bg_LR))\n",
        "#print('F1: %.3f' % f1_score(y_test, y_bg_LR))\n",
        "#print('Roc_auc: %.3f' % roc_auc_score(y_test, y_bg_LR))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Accuracy: %.3f' % accuracy_score(y_test, y_bg_LR_s))\n",
        "#print('Balanced Accuracy: %.3f' %balanced_accuracy_score(y_test, y_bg_LR_s))\n",
        "#print('Precision:', precision_score(y_test, y_bg_LR_s))\n",
        "#print('Recall:', recall_score(y_test, y_bg_LR_s))\n",
        "#print('F1: %.3f' % f1_score(y_test, y_bg_LR_s))\n",
        "#print('Roc_auc: %.3f' % roc_auc_score(y_test, y_bg_LR_s))"
      ],
      "metadata": {
        "id": "5JySY3zT8ubk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcRQ4nDpkgc5",
        "outputId": "0ea1577a-1d59-4ee2-f937-807d3ccc0258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.58264185\n",
            "Validation score: 0.761563\n",
            "Iteration 2, loss = 0.49677459\n",
            "Validation score: 0.760625\n",
            "Iteration 3, loss = 0.47916996\n",
            "Validation score: 0.774687\n",
            "Iteration 4, loss = 0.46521721\n",
            "Validation score: 0.775000\n",
            "Iteration 5, loss = 0.45384114\n",
            "Validation score: 0.777813\n",
            "Iteration 6, loss = 0.44285388\n",
            "Validation score: 0.781563\n",
            "Iteration 7, loss = 0.43454441\n",
            "Validation score: 0.780937\n",
            "Iteration 8, loss = 0.42378701\n",
            "Validation score: 0.780000\n",
            "Iteration 9, loss = 0.41430330\n",
            "Validation score: 0.785000\n",
            "Iteration 10, loss = 0.40406948\n",
            "Validation score: 0.785312\n",
            "Iteration 11, loss = 0.39420665\n",
            "Validation score: 0.780937\n",
            "Iteration 12, loss = 0.38600532\n",
            "Validation score: 0.782813\n",
            "Iteration 13, loss = 0.37683821\n",
            "Validation score: 0.785312\n",
            "Iteration 14, loss = 0.36676190\n",
            "Validation score: 0.782500\n",
            "Iteration 15, loss = 0.35586067\n",
            "Validation score: 0.785625\n",
            "Iteration 16, loss = 0.34743386\n",
            "Validation score: 0.790000\n",
            "Iteration 17, loss = 0.33847758\n",
            "Validation score: 0.787188\n",
            "Iteration 18, loss = 0.33131080\n",
            "Validation score: 0.791562\n",
            "Iteration 19, loss = 0.32049978\n",
            "Validation score: 0.787813\n",
            "Iteration 20, loss = 0.31137986\n",
            "Validation score: 0.790625\n",
            "Iteration 21, loss = 0.30637281\n",
            "Validation score: 0.788750\n",
            "Iteration 22, loss = 0.29513677\n",
            "Validation score: 0.786563\n",
            "Iteration 23, loss = 0.28953920\n",
            "Validation score: 0.778438\n",
            "Iteration 24, loss = 0.28038159\n",
            "Validation score: 0.792813\n",
            "Iteration 25, loss = 0.27193559\n",
            "Validation score: 0.789375\n",
            "Iteration 26, loss = 0.26689659\n",
            "Validation score: 0.784687\n",
            "Iteration 27, loss = 0.25861731\n",
            "Validation score: 0.786563\n",
            "Iteration 28, loss = 0.25007248\n",
            "Validation score: 0.787500\n",
            "Iteration 29, loss = 0.24484175\n",
            "Validation score: 0.794063\n",
            "Iteration 30, loss = 0.23568807\n",
            "Validation score: 0.791250\n",
            "Iteration 31, loss = 0.23172241\n",
            "Validation score: 0.794063\n",
            "Iteration 32, loss = 0.22342904\n",
            "Validation score: 0.788125\n",
            "Iteration 33, loss = 0.21609692\n",
            "Validation score: 0.783125\n",
            "Iteration 34, loss = 0.21474563\n",
            "Validation score: 0.783438\n",
            "Iteration 35, loss = 0.20980239\n",
            "Validation score: 0.790625\n",
            "Iteration 36, loss = 0.19748288\n",
            "Validation score: 0.780312\n",
            "Iteration 37, loss = 0.19251567\n",
            "Validation score: 0.776563\n",
            "Iteration 38, loss = 0.19162452\n",
            "Validation score: 0.783438\n",
            "Iteration 39, loss = 0.18252306\n",
            "Validation score: 0.783438\n",
            "Iteration 40, loss = 0.18022694\n",
            "Validation score: 0.777188\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Time: 51.9547970499998\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "a=time.perf_counter()\n",
        "clf = MLPClassifier(random_state=1,hidden_layer_sizes=(250,25), max_iter=200,early_stopping = True,verbose = True).fit(X_train_sw, y_train)\n",
        "b=time.perf_counter()\n",
        "print(\"Time: \" +str(b-a))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=time.perf_counter()\n",
        "clf_s = MLPClassifier(random_state=1,hidden_layer_sizes=(250,25), max_iter=200,early_stopping = True,verbose = True).fit(X_train_s, y_train)\n",
        "b=time.perf_counter()\n",
        "print(\"Time: \"+ str(b-a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpYrkzO481bF",
        "outputId": "20eb2b6e-99c5-44f2-cf29-1dbaa8eb768c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.51083528\n",
            "Validation score: 0.794687\n",
            "Iteration 2, loss = 0.43726661\n",
            "Validation score: 0.800312\n",
            "Iteration 3, loss = 0.42042763\n",
            "Validation score: 0.804688\n",
            "Iteration 4, loss = 0.40675433\n",
            "Validation score: 0.800312\n",
            "Iteration 5, loss = 0.39233983\n",
            "Validation score: 0.806250\n",
            "Iteration 6, loss = 0.37786431\n",
            "Validation score: 0.803438\n",
            "Iteration 7, loss = 0.36706024\n",
            "Validation score: 0.809063\n",
            "Iteration 8, loss = 0.35267126\n",
            "Validation score: 0.811562\n",
            "Iteration 9, loss = 0.34066092\n",
            "Validation score: 0.816562\n",
            "Iteration 10, loss = 0.32423725\n",
            "Validation score: 0.811562\n",
            "Iteration 11, loss = 0.31038351\n",
            "Validation score: 0.815625\n",
            "Iteration 12, loss = 0.29897692\n",
            "Validation score: 0.806562\n",
            "Iteration 13, loss = 0.28493885\n",
            "Validation score: 0.805937\n",
            "Iteration 14, loss = 0.27052472\n",
            "Validation score: 0.810937\n",
            "Iteration 15, loss = 0.25850772\n",
            "Validation score: 0.814063\n",
            "Iteration 16, loss = 0.24703755\n",
            "Validation score: 0.810625\n",
            "Iteration 17, loss = 0.23395321\n",
            "Validation score: 0.802813\n",
            "Iteration 18, loss = 0.22340041\n",
            "Validation score: 0.810937\n",
            "Iteration 19, loss = 0.21198704\n",
            "Validation score: 0.800000\n",
            "Iteration 20, loss = 0.19894527\n",
            "Validation score: 0.806250\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Time: 25.83490127699997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s_9UlTTxXFGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLeIW9ZhoF7j",
        "outputId": "0275ec2c-465f-4b57-b211-9c0e157c35a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.791\n",
            "Precision: 0.8016636340005199\n",
            "Recall: 0.7717717717717718\n",
            "F1: 0.786\n"
          ]
        }
      ],
      "source": [
        "y_mlp = clf.predict(X_test_sw)\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_mlp))\n",
        "#print('Balanced Accuracy: %.3f' %balanced_accuracy_score(y_test, y_mlp))\n",
        "print('Precision:', precision_score(y_test, y_mlp))\n",
        "print('Recall:', recall_score(y_test, y_mlp))\n",
        "print('F1: %.3f' % f1_score(y_test, y_mlp))\n",
        "#print('Roc_auc: %.3f' % roc_auc_score(y_test, y_mlp))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_mlp_s = clf_s.predict(X_test_s)\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_mlp_s))\n",
        "#print('Balanced Accuracy: %.3f' %balanced_accuracy_score(y_test, y_mlp_s))\n",
        "print('Precision:', precision_score(y_test, y_mlp_s))\n",
        "print('Recall:', recall_score(y_test, y_mlp_s))\n",
        "print('F1: %.3f' % f1_score(y_test, y_mlp_s))\n",
        "#print('Roc_auc: %.3f' % roc_auc_score(y_test, y_mlp_s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJmVHk5R8-m2",
        "outputId": "95cf83c4-6084-4b91-f1f4-f7b347b40565"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.817\n",
            "Precision: 0.8060928433268859\n",
            "Recall: 0.8343343343343343\n",
            "F1: 0.820\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}